{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Initial Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Atoosa Rashid** \n",
    "\n",
    "[GitHub](https://github.com/atoosa-r/) | [LinkedIn](https://www.linkedin.com/in/atoosarashid/) \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Data Dictionary](#data-dictionary)\n",
    "- [Initial Exploration](#initial-exploration)\n",
    "- [Data Cleaning](#data-cleaning)\n",
    "  - [Dropping Unnecessary Columns](#dropping-unnecessary-columns)\n",
    "  - [Handling Missing Values](#handling-missing-values)\n",
    "  - [Removing Duplicates](#removing-duplicates)\n",
    "- [Final Remarks](#final-remarks)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "This notebook focuses on cleaning and preparing the Spotify streaming data for analysis. The process involves importing the data, examining its structure, handling missing values, removing irrelevant columns, and eliminating duplicates. By the end of this notebook, the dataset will be ready for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries: \n",
    "\n",
    "import numpy as np                   \n",
    "import pandas as pd                   \n",
    "import matplotlib.pyplot as plt       \n",
    "import seaborn as sns                 \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "# Path to the folder containing the JSON files (fill in with your own folder path)\n",
    "directory_path = 'path_to_your_files/JSON Files'\n",
    "\n",
    "# Getting a list of all JSON files in the directory\n",
    "json_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.json')]\n",
    "\n",
    "# Combining all JSON files into a single dataframe (df)\n",
    "df = pd.concat([pd.read_json(file) for file in json_files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides detailed insights into your Spotify streaming history. Below is a description of each field:\n",
    "\n",
    "| **Field Name**                     | **Description**                                                                                         | **Example**                              |\n",
    "|------------------------------------|---------------------------------------------------------------------------------------------------------|------------------------------------------|\n",
    "| **ts**                             | Timestamp indicating when the track stopped playing (UTC). Format: YYYY-MM-DDTHH:MM:SSZ.               | `2025-01-01T13:30:30Z`                   |\n",
    "| **platform**                       | Platform or device used for streaming (e.g., operating system and device model).                       | `iOS 14.2 (iPhone10,6)`                  |\n",
    "| **ms_played**                      | Total milliseconds the track was played.                                                               | `2205`                                   |\n",
    "| **conn_country**                   | Two-letter country code where the stream occurred.                                                     | `CA`                                     |\n",
    "| **ip_addr**                        | IP address logged during the stream.                                                                   | `72.143.202.158`                         |\n",
    "| **master_metadata_track_name**     | Name of the track streamed.                                                                            | `Lucky`                                |\n",
    "| **master_metadata_album_artist_name** | Name of the artist or band.                                                                            | `H.E.R.`                               |\n",
    "| **master_metadata_album_album_name** | Name of the album containing the track.                                                                | `Lucky`                                |\n",
    "| **spotify_track_uri**              | Spotify URI uniquely identifying the track. Format: `spotify:track:<base-62 string>`.                  | `spotify:track:3kdyQO3jkZiUOtvoNGwOjw`   |\n",
    "| **episode_name**                   | Name of the podcast episode (if applicable).                                                           | `Breaking Down the Day's News`           |\n",
    "| **episode_show_name**              | Name of the podcast show (if applicable).                                                              | `The Current`                            |\n",
    "| **spotify_episode_uri**            | Spotify URI uniquely identifying the podcast episode. Format: `spotify:episode:<base-62 string>`.      | `spotify:episode:abc123`                 |\n",
    "| **audiobook_title**                | Title of the audiobook (if applicable).                                                                | `The Great Gatsby`                       |\n",
    "| **audiobook_uri**                  | Spotify URI uniquely identifying the audiobook. Format: `spotify:audiobook:<base-62 string>`.          | `spotify:audiobook:123abc`               |\n",
    "| **audiobook_chapter_uri**          | Spotify URI identifying a specific audiobook chapter.                                                  | `spotify:audiobook:chapter:xyz456`       |\n",
    "| **audiobook_chapter_title**        | Title of the audiobook chapter (if applicable).                                                        | `Chapter 1: The Beginning`               |\n",
    "| **reason_start**                   | Reason why the track started (e.g., `fwdbtn`, `trackdone`).                                            | `fwdbtn`                                 |\n",
    "| **reason_end**                     | Reason why the track ended (e.g., `fwdbtn`, `endplay`).                                                | `fwdbtn`                                 |\n",
    "| **shuffle**                        | Indicates if shuffle mode was used during playback (`True`/`False`/`null`).                            | `False`                                  |\n",
    "| **skipped**                        | Indicates if the track was skipped (`True`/`False`/`null`).                                             | `False`                                  |\n",
    "| **offline**                        | Indicates if the track was played offline (`True`/`False`/`null`).                                     | `False`                                  |\n",
    "| **offline_timestamp**              | Timestamp of when offline mode was used, if applicable.                                                | `2025-01-01T14:00:00Z`                   |\n",
    "| **incognito_mode**                 | Indicates if the track was played during a private session (`True`/`False`/`null`).                    | `False`                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of our df\n",
    "\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the full df and all columns \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying data types of all the columns\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data type of the `ts` column to a datetime object\n",
    "df['ts'] = pd.to_datetime(df['ts'])\n",
    "\n",
    "# Reviewing the full timeframe of the data\n",
    "earliest_date = df['ts'].min()\n",
    "latest_date = df['ts'].max()\n",
    "\n",
    "print(f\"The dataset covers the time period from {earliest_date} to {latest_date}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Unnecessary Columns:\n",
    "   - Columns related to audiobooks (`audiobook_*`) and episodes (`episode_*`) will be removed as they are not relevant for our analysis, which focuses on music streaming activity.\n",
    "   - The IP Address column (`ip_addr`) will also be dropped since we will not be working with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "\n",
    "columns_to_drop = [\n",
    "    'episode_name', 'episode_show_name', 'spotify_episode_uri', \n",
    "    'audiobook_title', 'audiobook_uri', 'audiobook_chapter_uri', \n",
    "    'audiobook_chapter_title', 'ip_addr'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____ \n",
    "\n",
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing all missing values found in the columns\n",
    "\n",
    "missing_values_summary = df.isnull().sum()\n",
    "\n",
    "print(\"\\n Missing Values Summary Report\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values:**\n",
    "\n",
    "- Metadata columns:\n",
    "    - Missing values for `master_metadata_track_name`, `master_metadata_album_artist_name`, and `master_metadata_album_album_name` suggest a lack of track-specific data for certain entries. This could be non-music streams (e.g., advertisements) or unlogged metadata or unavailable songs.\n",
    "    - These missing values represent less than 0.2% of the total dataset, so their impact on analysis is minimal.\n",
    "    - Due to their minimal impact and lack of relevant information, these rows will be dropped.\n",
    "- The `offline` column:\n",
    "    - Indicates whether a session occurred offline (True/False).\n",
    "    - Has minimal missing data.\n",
    "- The `offline_timestamp` column:\n",
    "    - Records timestamps for offline sessions.\n",
    "    - Has a high proportion of missing values (mostly for online sessions where it is irrelevant).\n",
    "    - Online sessions do not require `offline_timestamp`, so gaps are expected and not problematic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with missing values in metadata specified columns\n",
    "missing_rows = df[df[['master_metadata_track_name', \n",
    "                      'master_metadata_album_artist_name', \n",
    "                      'master_metadata_album_album_name']].isnull().any(axis=1)]\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "missing_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the specified columns , since they hold no value \n",
    "df = df.dropna(subset=['master_metadata_track_name', \n",
    "                       'master_metadata_album_artist_name', \n",
    "                       'master_metadata_album_album_name']).reset_index(drop=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial check for duplicates\n",
    "\n",
    "print(f\"Duplicates found: {df.duplicated().any()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting all duplicate rows (including all occurrences of duplicates)\n",
    "\n",
    "duplicated_rows = df[df.duplicated(keep=False)]\n",
    "print(f\"Total duplicate rows: {len(duplicated_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the individual duplicated rows\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "duplicated_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows that will actually be removed\n",
    "\n",
    "rows_to_remove = df.duplicated(keep='first').sum()\n",
    "print(f\"Number of rows to be removed: {rows_to_remove}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reviewing, these duplicates seem to be identical (timestamps, IPs, etc.), so we can go ahead and remove them from our df:\n",
    "\n",
    "rows_before = len(df)  \n",
    "df = df.drop_duplicates() \n",
    "rows_after = len(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying after removing duplicates\n",
    "\n",
    "print(f\"Number of rows removed: {rows_before - rows_after}\")\n",
    "print(f\"Final shape of DataFrame: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned df to a CSV file\n",
    "#df.to_csv('/_spotify_streaming_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Final Remarks\n",
    "\n",
    "- The dataset has been cleaned and prepared for analysis, with irrelevant columns dropped, missing values handled, and duplicates removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
